<?xml version="1.0" encoding="utf8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title></title><link href="htttps://pietronvll.github.io/" rel="alternate"></link><link href="htttps://pietronvll.github.io/atom.xml" rel="self"></link><id>htttps://pietronvll.github.io/</id><updated>2024-12-16T12:00:00+01:00</updated><entry><title>The Operator Way</title><link href="htttps://pietronvll.github.io/the-operator-way.html" rel="alternate"></link><published>2024-12-16T12:00:00+01:00</published><updated>2024-12-16T12:00:00+01:00</updated><author><name>Pietro Novelli</name></author><id>tag:pietronvll.github.io,2024-12-16:/the-operator-way.html</id><summary type="html">A paradigm to rethink dynamical processes</summary><content type="html">&lt;h2&gt;A paradigm to rethink dynamical processes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to sensibly choose the next word to complete this unfinished â€” ?&lt;/li&gt;
&lt;li&gt;How much time it takes, on average, for a new experimental drug to bind to the human organism?&lt;/li&gt;
&lt;li&gt;How can self-driving vehicles drive safely at all?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These seemingly random questions are tied together by the common thread of understanding how things &lt;em&gt;evolve&lt;/em&gt;. Words follow each other in patterns that create meaning. Molecules navigate the human body according to the laws of atomic physics. And traffic flows through our cities in largely predictable patterns, determining our driving actions.&lt;/p&gt;
&lt;p&gt;Understanding evolution in this broad sense is no small feat. I&amp;rsquo;ll take up the challenge by describing a paradigm that characterizes all sorts of evolving processes, and no, this is not yet another take on &lt;a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"&gt;transformers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The roots of this approach â€” which I&amp;rsquo;ll call &lt;em&gt;the operator way&lt;/em&gt; â€” stretch all the way back to the dawn of quantum mechanics. However, we need to fast-forward to the early 2000s before a critical mass of researchers (from the fields of stochastic processes and applied math) started getting serious with it. Nowadays, the operator way is making its way into the AI era, where its power and elegance are truly brought to bear.&lt;/p&gt;
&lt;p&gt;Unfortunately, the most insigthful ideas behind the operator way are hidden beneath a thick coat of abstractions, making it a tough topic to enter. This post is an attempt at describing it in plain, non-technical terms, so that as many as possible can appreciate it and the fresh new perspective it offers on the study of sequential and evolving processes.&lt;/p&gt;
&lt;h2 id="dealing-with-change"&gt;Dealing with change&lt;/h2&gt;
&lt;p&gt;Whenever we observe an evolving process, we might find ourselves wondering what&amp;rsquo;s the rule that ties our observations together, if any. In other words, the fact that $x_{t + 1}$ was observed &lt;em&gt;after&lt;/em&gt; $x_{t}$ is just a coincidence, or is it the result of a predictable relationship? &lt;/p&gt;
&lt;p&gt;Oftentimes, such a relationship exists. Let&amp;rsquo;s take a look at a couple of examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Planetary motion&lt;/strong&gt; (&lt;em&gt;deterministic change&lt;/em&gt;)
Before Einstein&amp;rsquo;s relativity, the best description of planetary orbits came from &lt;a href="https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion"&gt;Newton&amp;rsquo;s laws of motion&lt;/a&gt;. These are &lt;a href="https://en.wikipedia.org/wiki/Differential_equation"&gt;differential equations&lt;/a&gt; which, given the positions and velocities of planets at at any time $t$, can be solved to predict their positions and velocities at a later time $t&amp;rsquo;$. This is an example of &lt;em&gt;deterministic&lt;/em&gt; dynamics, where each initial condition leads to a unique and well-defined evolution of the system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language&lt;/strong&gt; (&lt;em&gt;stochastic change&lt;/em&gt;)
In &lt;a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"&gt;A Mathematical Theory of Communication&lt;/a&gt;, Claude Shannon described how the English language can be approximated by producing letters or words according to their relative frequencies in the natural language. This is a &lt;em&gt;stochastic&lt;/em&gt; change, where the next state $x_{t + 1}$ (the next word or letter) isn&amp;rsquo;t certain. However, despite being uncertain, the next state abides to a definite conditional probability law $\mathbb{P}(x_{t + 1} | x_{t})$. Incidentally, this principle is also at the core of the large language models fueling the GPTs, Claudes, and Geminis out there.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In machine learning and AI, we make use of observations and data to extract &lt;em&gt;models&lt;/em&gt; describing their inter-relationships. In the case of evolving processes, there are three good reasons to go after a good &lt;em&gt;dynamical&lt;/em&gt; model: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interpretability&lt;/strong&gt;: understanding &lt;em&gt;how&lt;/em&gt; $x_{t} \to x_{t + 1}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: &lt;em&gt;forecasting&lt;/em&gt; $x_{t + 1}$ or $\mathbb{P}[x_{t + 1}]$, having observed $x_{t}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: nudging the system toward a desired state $x_{t +1}$, starting from $x_{t}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="linear-models"&gt;The case of &lt;em&gt;linear&lt;/em&gt; models&lt;/h3&gt;
&lt;p&gt;A set of observations $x_{t}$ is said to evolve &lt;em&gt;linearly&lt;/em&gt; when its rate of change does not depend on the current value $x_{t}$. As an example of linear evolution, imagine that at any time $t$ we lend some amount of money $x_{t}$ at a fixed rate of 3% [^1]. After one year, the value of our investment $x_{t + 1}$ will be increased by 3%, irrespectively of the initial amount invested. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;A bit of math.&lt;/strong&gt; So far, I haven&amp;rsquo;t bothered defining what our observations $x$ &lt;em&gt;are&lt;/em&gt;. In the following, I will be appeased assuming that any &lt;strong&gt;arbitrary&lt;/strong&gt; linear combination $\alpha x + \beta y$ is still a meaningful observation. This assumption can be unpacked into two separate requirements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It makes sense to multiply $x$ with a number $\alpha$. For example, if $x$ is the amount of our investment in millions, $1000000 \cdot x$ is  our investment in dollars.&lt;/li&gt;
&lt;li&gt;Arbitrary sums of two observations $x$ and $y$ yield another observation &amp;ldquo;of the same kind&amp;rdquo;. As the previous requirement might sound abstract, let me explicitly give you an example when this is &lt;strong&gt;not&lt;/strong&gt; satisfied. A lot of data (molecular conformations, social networks, power grids) come in the form of graphs evolving in time, but what does it means to add two graphs together, and how on earth summing two graphs give me another graph?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finite-dimensional &lt;a href="https://en.wikipedia.org/wiki/Euclidean_vector"&gt;euclidean vectors&lt;/a&gt; clearly satisfy both conditions, but notice that also &lt;em&gt;functions&lt;/em&gt; $f:\mathbb{R}^{d}\to \mathbb{R}$ can be linearly combined to get&amp;hellip; yet another function. If $x_{t}$ satisfies the assumptions above and follows a &lt;strong&gt;linear&lt;/strong&gt; evolution, it exists a rule $\mathsf{T}$ such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$x_{t + 1} = \mathsf{T}x_{t}$,&lt;/li&gt;
&lt;li&gt;$\mathsf{T}(x + y) = \mathsf{T}x + \mathsf{T}y$, &lt;/li&gt;
&lt;li&gt;$\mathsf{T}(\alpha x) = \alpha \mathsf{T}x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;rsquo;ve dubbed such a rule $\mathsf{T}$ as it &lt;strong&gt;T&lt;/strong&gt;ransfers $x_{t}$ to $x_{t + 1}$. When $x \in \mathbb{R}^{d}$ is an Euclidean vector, then $\mathsf{T}$ is a $d\times d$ real matrix. In more exotic cases, for example when $x_{t}$ is a &lt;em&gt;function&lt;/em&gt;, I&amp;rsquo;ll stick to the mathematicians&amp;rsquo; naming conventions and refer to $\mathsf{T}$ as a &lt;strong&gt;&lt;em&gt;linear operator&lt;/em&gt;&lt;/strong&gt;. If you are unfamiliar with the concepts of linear transformations, this video by 3Blue1Brown provides an excellent visual explaination of them &lt;iframe width="560" height="315" style="margin:auto; display:block;" src="https://www.youtube.com/embed/kYB8IZa5AuE?si=i1IpoJUqqEMF5wWA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If $x_{t}$ evolves linearly, the triad of interpretability, prediction, and control has been completely sorted out by the scientific community, which came up a vast body of elegant and efficient techniques to address these tasks. &lt;/p&gt;
&lt;p&gt;Nonlinear evolutions, on the other hand, are a different beast, with interpretability and control becoming significantly more challenging. This aside on linear systems will be extremely important as we progress. For now, bear with me.&lt;/p&gt;
&lt;h3&gt;What does &lt;em&gt;learning&lt;/em&gt; a dynamics means, anyway?&lt;/h3&gt;
&lt;p&gt;The taxonomy of models for evolving processes is quite intricate. In the two examples discussed &lt;a href="#dealing-with-change"&gt;above&lt;/a&gt;, we&amp;rsquo;ve seen that a dynamical model can be specified by, for example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of differential equations.&lt;/li&gt;
&lt;li&gt;A transition probability $\mathbb{P}(x_{t + 1} | x_{t})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many more options, and &lt;em&gt;before even trying&lt;/em&gt; to throw some AI at our data hoping to get a model out of it, we have to define a suitable target, that is a mathematical entity that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can be approximated from data&lt;/li&gt;
&lt;li&gt;Can be represented, stored, and queried in a computer&lt;/li&gt;
&lt;li&gt;Unlocks at least one of the tasks in the triad (Interpretability, Prediction, Control)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Coming back to our examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href="https://en.wikipedia.org/wiki/Symbolic_regression"&gt;symbolic regression&lt;/a&gt; the target is an analytical equation, which can be approximated from data using tools ranging from genetic algorithms to deep learning. Symbolic regression is the perfect tool to learn deterministic equations such as the aforementioned Newton laws, and it provides extremely interpretable models. Astrophysicists are &lt;a href="https://www.ias.edu/news/astrophysicists-weigh-galaxy-clusters-artificial-intelligence"&gt;already&lt;/a&gt; deploying it succesfully in cutting-edge research use cases.&lt;/li&gt;
&lt;li&gt;As I&amp;rsquo;ve noted already, large language models approximate a transition probability  $\mathbb{P}(x_{t + 1} | x_{t}, \ldots, x_{0})$ and leverage expressive neural-network architectures[^2] on huge amounts of data to get accurate predictions. Interpreting and controlling language models, however, it is still a formidable task â€” ever heard of &lt;em&gt;alignment&lt;/em&gt; and &lt;a href="https://transformer-circuits.pub"&gt;&lt;em&gt;mechanistic interpretability&lt;/em&gt;&lt;/a&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Operator Way&lt;/h2&gt;
&lt;p&gt;The operator way offers a different option, motivated by the following widely-applicable observation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It &lt;strong&gt;always&lt;/strong&gt; exists a suitable encoding $\phi$ of our observations $x_{t}$ such that $\phi(x_{t})$ evolves &lt;strong&gt;linearly&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While the previous fact is easy to state, its proof requires a bunch of technicalities. Let&amp;rsquo;s set them aside for a moment, and focus on what it &lt;em&gt;implies&lt;/em&gt;, instead. As I&amp;rsquo;ve explained above, when the evolution $\phi(x_{t}) \to \phi(x_{t + 1})$ is &lt;em&gt;linear&lt;/em&gt; it means it exists an &lt;em&gt;operator&lt;/em&gt; $\mathsf{T}$ such that $\phi(x_{t + 1}) = \mathsf{T}\phi(x_{t})$. So, provided the right representation $\phi$, the operator $\mathsf{T}$ encodes the law of evolution of $x_{t}$. &lt;/p&gt;
&lt;p&gt;Now, let me remark that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathsf{T}$ is a &lt;em&gt;definite mathematical entity&lt;/em&gt;, which can be approximated from data by minimizing the linearization error $\Vert \phi(x_{t +1}) - \mathsf{T}\phi(X_{t})\Vert$. Rings any bell?&lt;/li&gt;
&lt;li&gt;The existence of an appropriate linearizing transformation $\phi$ can be proved on &lt;em&gt;extremely&lt;/em&gt; general grounds, and applies to both deterministic systems via the so-called &lt;a href="https://en.wikipedia.org/wiki/Composition_operator"&gt;Koopman operator&lt;/a&gt;, and stochastic systems via the Transfer operator.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The operator way acknowledges that dynamical processes can be &lt;em&gt;linearized&lt;/em&gt; by encoding the dynamical data in a suitable representation $\phi$, and identifies them with the resulting &lt;em&gt;linear&lt;/em&gt; law of evolution $\mathsf{T}$. Recalling that for linearly evolving processes such as $\phi(x_{t})$, the whole triad of Interpretability, Prediction, and Control can be addressed simultaneously, the operator way becomes a &lt;em&gt;really compelling&lt;/em&gt; modeling choice. &lt;/p&gt;
&lt;h2&gt;Cool! How do we learn it?&lt;/h2&gt;
&lt;p&gt;In the introductory paragraph I&amp;rsquo;ve argued that the operator way can be happily married to machine learning. How? In a typical machine learning scenario, we have a stream of data $x_{t}, x_{t + 1}, \ldots$ coming at us, and we want to learn a &lt;em&gt;model&lt;/em&gt; describing the evolving patters of these observations. Following the insights from the operator way, we posit that it exists&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;em&gt;representation&lt;/em&gt; $\phi(x)$ of the data, and&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;latent linear model&lt;/em&gt; $\mathsf{T}$ such that $\phi(x_{t +1}) = \mathsf{T}\phi(x_{t}).$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, suppose someone gave us $\phi$. In this case, $\mathsf{T}$ can be approximated from data by minimizing the sum of the squared errors ${\rm Error}(\mathsf{T}) = \sum_{t} \Vert \phi(x_{t +1}) - \mathsf{T}\phi(x_{t})\Vert^{2}$. For any reader with some data-science experience: this is just an instance of multivariate linear regression, where $\phi(x_{t + 1})~\big(\phi(x_{t})\big)$ are the dependent (independent) variables, respectively.&lt;/p&gt;
&lt;p&gt;When $\phi$ is known, approximating $\mathsf{T}$ from data is a well-specified problem, which can be solved efficiently. On the other hand, when a suitable transformation $\phi$ is unknown, one can bootstrap the whole process learning both $\phi$ and $\mathsf{T}$ from data. This is a more challenging scenario than approximating $\mathsf{T}$ alone, but a number of compelling options are already available[^3].&lt;/p&gt;
&lt;p&gt;There is a catch in all of this. The catch lies in the nature of the linearizing representation $\phi$. The theory says it exists, but it also says that it might be &lt;em&gt;infinite dimensional&lt;/em&gt; â€” if you&amp;rsquo;re uncomfortable with infinities, just picture yourself $\phi(x_{t})$ as an euclidean vector with an infinite number of components. Pretty impractical to store in a computer. &lt;/p&gt;
&lt;p&gt;Luckily, this is just a problem &lt;em&gt;in theory&lt;/em&gt;, and can be effectively circumvented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.09522"&gt;Kernel methods&lt;/a&gt;, while not fashionable as they used to be, offer a viable framework to learn with infinite-dimensional representations.&lt;/li&gt;
&lt;li&gt;Finite-dimensional representations usually incur little errors, when learned from data with one of the approaches mentioned above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we approach the end of this post, allow me just few more lines to describe how the operator way originated in the early days of quantum mechanics. You&amp;rsquo;ll be surprised (hopefully even awed) to see how it emerged from a &lt;strong&gt;completely different way of thinking&lt;/strong&gt; evolving processes!&lt;/p&gt;
&lt;h2&gt;The Operator Way and Quantum Mechanics.&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s 1931, the foundations of quantum mechanics have been laid down just few years ago. In this novel view of nature, a physical system is entirely described by its so-called &lt;em&gt;state&lt;/em&gt; $x$, which in practice is an Euclidean vector or a function[^4]. The state $x_{t}$ provides information on the system, at a given time $t$. Nowadays we have &lt;a href="https://en.wikipedia.org/wiki/Quantum_tomography"&gt;quantum state tomography&lt;/a&gt;, but in 1931 the experimental physicists couldn&amp;rsquo;t measure the state of a quantum system directly. Rather, they were able to measure only some properties of it. Since the state $x_{t}$ encodes every information of the system, what experimentalists could observe in their labs were &lt;em&gt;functions&lt;/em&gt; of the state $f(x_{t})$, oftentimes called &lt;em&gt;observables&lt;/em&gt;. Notice that this way of thinking is not restricted to atomic physics, and can be applied to many diverse domains. For example, have you ever thought that Google Maps don&amp;rsquo;t actually need to know precisely where you are? It only needs measurements of the position of a bunch of GPS satellites to triangulate you, and give an estimate of your position.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s jump back to 1931, when the mathematician Bernard Koopman noticed that thinking in terms of observables can be useful even outside the domain of quantum mechanics. In his own words: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In recent years the theory of Hilbert space and its linear transformations has come into prominence. [&amp;hellip;] the importance of the theory in quantum mechanics is known to all. It is the object of &lt;a href="https://www.pnas.org/doi/10.1073/pnas.17.5.315"&gt;this note&lt;/a&gt; to outline certain investigations of our own in which the domain of this theory has been extended in such a way as to include classical Hamiltonian mechanics, or, more generally, systems defining a steady n-dimensional flow of a fluid of positive density.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s follow this hunch, and assume that the evolution of the state $x_{t} \to x_{t + 1}$ is determined by a &lt;em&gt;possibly non-linear&lt;/em&gt; rule $S$ so that $x_{t + 1} = S(x_{t})$. Now let&amp;rsquo;s pick any observable $f$. Recalling that $f$ is just a function of the state, we have that $f(x_{t + 1}) = f(S(x_{t}))$. In summary, the original observable $f$ evaluated at $x_{t + 1}$ equates to the composition of $f$ with the dynamical rule $S$, evaluated at $x_{t}$. Using a common notation let me write $f \circ S$ as the composition of $f$ and $S$. Notice that $f\circ S$ is just another function of the state, that is, &lt;em&gt;another observable&lt;/em&gt;. This composition property is &lt;a href="#linear-models"&gt;linear&lt;/a&gt;, meaning that if take an arbitrary linear combination of observables $\alpha f + \beta g$, the composition with $S$ preserves it:&lt;/p&gt;
&lt;p&gt;$$(\alpha f + \beta g)(x_{t +1}) = \alpha f(x_{t + 1}) + \beta g(x_{t +1}) = \alpha (f\circ S)(x_{t}) + \beta (g \circ S)(x_{t}) = \big[(\alpha f + \beta g) \circ S \big](x_{t}).$$ &lt;/p&gt;
&lt;p&gt;So, the evolution of observables can be expressed as a &lt;em&gt;linear rule&lt;/em&gt; mapping observables to observables, let me denote it $\mathsf{T}_{S}$. The operator $\mathsf{T}_{S}$ acts on an observable $f$ as&lt;/p&gt;
&lt;p&gt;$$f(x_{t + 1}) = (\mathsf{T}_{S}f)(x_{t}) := f(S(x_{t})).$$&lt;/p&gt;
&lt;p&gt;If we happen to know $\mathsf{T}_{S}$, we automatically have a &lt;em&gt;linear&lt;/em&gt; description of the evolution process &lt;em&gt;as a whole&lt;/em&gt;, for any arbitrary observable. This is the operator way, once again, quantum-mechanics edition. &lt;/p&gt;
&lt;p&gt;To actually show that this formulation of the operator way is equivalent to the first one I gave, we need just a small additional step. We need to conceive observables as abstract objects $f$, which can be queried at any point $x$ by an &lt;em&gt;evaluation&lt;/em&gt; object[^5], which I&amp;rsquo;ll denote â€” for NO REASON at all ðŸ™ˆ â€” $\phi(x)$, and let me denote this evaluation procedure as a scalar product $f(x) = \langle f, \phi(x)\rangle$. In this notation,&lt;/p&gt;
&lt;p&gt;$$f(x_{t + 1}) = \langle f, \phi(x_{t + 1})\rangle = (\mathsf{T}_{S}f)(x_{t}) = \langle \mathsf{T}_{S}f, \phi(x_{t})\rangle = \langle f, \mathsf{T}_{S}^{*}\phi(x_{t})\rangle$$&lt;/p&gt;
&lt;p&gt;If you swallowed this, recalling that the relation above holds for any observable $f$, we have that&lt;/p&gt;
&lt;p&gt;$$\phi(x_{t + 1}) = \mathsf{T}_{S}^{*}\phi(x_{t})$$&lt;/p&gt;
&lt;h2&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;ve made up this far, thank you! Writing a blog post was a first time for me; if you have any comments, questions, or complaints feel free to reach out, I&amp;rsquo;ll be happy to answer! &lt;/p&gt;
&lt;p&gt;In a tentative next post, I&amp;rsquo;d like to present the most interesting applications of the operator way to date, spanning areas such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Molecular dynamics&lt;/li&gt;
&lt;li&gt;Robotics&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Climate Science&lt;/li&gt;
&lt;li&gt;Neural Networks&amp;rsquo; interpretability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned!&lt;/p&gt;
&lt;p&gt;[^1]: For example through a government bond.
[^2]: Like transformers or state space models.
[^3]: See, for example, &lt;a href="https://arxiv.org/abs/1712.09707"&gt;Lush et al.&lt;/a&gt; using an autoencoding scheme, &lt;a href="https://arxiv.org/abs/1710.06012"&gt;Mardt et al.&lt;/a&gt; using an approach linked to canonical correlation analyisis, or &lt;a href="https://arxiv.org/abs/2307.09912"&gt;these&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2407.01171"&gt;methods&lt;/a&gt; by yours truly.
[^4]: Physicists, often refer to the state as the &lt;em&gt;wave-function&lt;/em&gt; of the system.
[^5]: I&amp;rsquo;m being extra-sloppy here, apologies to the analysts.&lt;/p&gt;</content></entry></feed>